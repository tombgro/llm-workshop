{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c4f0e8-c319-4ea1-a73b-8c4866cf639f",
   "metadata": {},
   "source": [
    "# Notebook for LLM workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3194a575-8f3d-464e-9b4d-cd0669f8555a",
   "metadata": {},
   "source": [
    "## Part 1. Examining and fine-tuning BERT\n",
    "\n",
    "Based on a tutorial from [Kaggle](https://www.kaggle.com/code/harshjain123/bert-for-everyone-tutorial-implementation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "780624af-77f9-4f70-9c6b-a741fbaa11ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe851d2a-895a-424a-a10d-1cffccd2262b",
   "metadata": {},
   "source": [
    "### Getting BERT and its tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8890e8f-7617-400b-9e8e-5cad32af95ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommigro/miniconda3/envs/llm-cpu/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import the Transformers library and download BERT\n",
    "\n",
    "from transformers import AutoModel, BertTokenizerFast, AdamW\n",
    "\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bf4536a-0b3f-49d5-a1f9-729426213692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fce4e5a-9693-4090-949a-cc8caa2be785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dda3af-0ace-429c-8073-4a7949174c6a",
   "metadata": {},
   "source": [
    "### Defining the BERT-based sentence classifier architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2df8d0c1-a76d-4472-aaf6-ea7afa7005da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        # BERT part\n",
    "        self.bert = bert \n",
    "        \n",
    "        # dropout layer (random removing of components during training for improving performance)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    # define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        \n",
    "        # pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "\n",
    "        # run BERT output through dense layer 1\n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        # apply relu activation function\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # apply dropout\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "      \n",
    "        # apply softmax activation (get probability distribution across target classes)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a82e9741-d715-41ba-a415-36b9f71e3ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_Arch(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52cb0f03-8c7d-4d32-8ff0-cd736c3c2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put model to GPU if available\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed5f3c23-029d-4b3d-adb5-3ef6e202eacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT_Arch(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018fb65b-0324-4cf2-b31b-f8c112cd0cae",
   "metadata": {},
   "source": [
    "### Getting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7d5b3d3-6af3-47cb-9dcb-597cbc9aa212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pre-labeled spam detection dataset\n",
    "df = pd.read_csv(\"spamdata_v2.csv\")\n",
    "\n",
    "# split train dataset into train, validation and test sets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
    "                                                                    random_state=2024, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['label'])\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2024, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)\n",
    "\n",
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 25,\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = 25,\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 25,\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e365c12b-cc3c-4d5d-8b8a-df0a02ba1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lists to tensors\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f6f44f3-0978-474a-b284-a181551b8045",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] u can call now... [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n"
     ]
    }
   ],
   "source": [
    "# train_mask is used for ignoring input padding in attention\n",
    "\n",
    "print(tokenizer.decode(train_seq[0]))\n",
    "print(train_mask[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60782afc-275a-4fce-9701-793e82414372",
   "metadata": {},
   "source": [
    "### Training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d8a40a2-f1c9-43a6-a9ec-63c1b6f0bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors for training data\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for randomly sampling data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for training data\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors for validation data\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for randomly sampling data during validation\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "880c18ed-64cc-434c-9411-d091bab58290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all BERT parameters (training only linear layers outside of BERT)\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a93a2932-25e6-43a3-914b-5187f9da4303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommigro/miniconda3/envs/llm-cpu/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),lr = 1e-5) \n",
    "\n",
    "#compute the class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "\n",
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# push to GPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58916c01-e521-49c1-8f6a-0d9a18770627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "\n",
    "    # put model in training mode (apply Dropout)\n",
    "    model.train()\n",
    "\n",
    "    # initialize loss and accuracy\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save model predictions\n",
    "    total_preds = []\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # print progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        \n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    " \n",
    "        sent_id, mask, labels = batch\n",
    "        \n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "      # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds\n",
    "\n",
    "\n",
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    \n",
    "    print(\"\\nEvaluating...\")\n",
    "  \n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "        \n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            \n",
    "            # # Calculate elapsed time in minutes.\n",
    "            # elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ca204be-ab2e-4d92-a386-f5c15ab2918f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.680\n",
      "Validation Loss: 0.671\n",
      "\n",
      " Epoch 2 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.652\n",
      "Validation Loss: 0.630\n",
      "\n",
      " Epoch 3 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.626\n",
      "Validation Loss: 0.606\n",
      "\n",
      " Epoch 4 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.600\n",
      "Validation Loss: 0.583\n",
      "\n",
      " Epoch 5 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.582\n",
      "Validation Loss: 0.566\n",
      "\n",
      " Epoch 6 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.561\n",
      "Validation Loss: 0.551\n",
      "\n",
      " Epoch 7 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.535\n",
      "Validation Loss: 0.523\n",
      "\n",
      " Epoch 8 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.521\n",
      "Validation Loss: 0.503\n",
      "\n",
      " Epoch 9 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.503\n",
      "Validation Loss: 0.487\n",
      "\n",
      " Epoch 10 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.490\n",
      "Validation Loss: 0.467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_85425/3189381092.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91       724\n",
      "           1       0.48      0.88      0.62       112\n",
      "\n",
      "    accuracy                           0.86       836\n",
      "   macro avg       0.73      0.87      0.77       836\n",
      "weighted avg       0.91      0.86      0.87       836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "# for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    # train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    # evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    # save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')\n",
    "\n",
    "\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))\n",
    "\n",
    "# free cached GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d5ac5-1b5f-4847-ae74-ef317a5cdeb9",
   "metadata": {},
   "source": [
    "## Part 2. Sentence similarities with SentenceBERT\n",
    "\n",
    "Based on [SentenceBERT documentation](https://sbert.net/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54327ee4-45b3-4ae4-aff4-4eb39695524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76c5817d-5f91-4cab-85a3-8c969b1d954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0aa19710-0d50-49c0-874f-2bc0e2d2e3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.4128, 0.5024, 0.7230, 0.4712],\n",
      "        [0.4128, 1.0000, 0.3096, 0.4911, 0.7424],\n",
      "        [0.5024, 0.3096, 1.0000, 0.4433, 0.3470],\n",
      "        [0.7230, 0.4911, 0.4433, 1.0000, 0.6147],\n",
      "        [0.4712, 0.7424, 0.3470, 0.6147, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "# sentences to encode\n",
    "sentences = [\n",
    "    \"I am happy.\",\n",
    "    \"I am sad.\",\n",
    "    \"I am content.\",\n",
    "    \"I am not happy.\",\n",
    "    \"I am not sad.\"\n",
    "]\n",
    "\n",
    "# embeddings\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# embedding similarities\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a102f91-2805-4443-8b24-de3f729fbeef",
   "metadata": {},
   "source": [
    "## Part 3. Examining Llama2 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8452ce57-1fec-4605-b043-b985d8152034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dd9574b-89b2-4b82-9b64-15abb47038d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No GPU found. A GPU is needed for quantization.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Pre-trained LLAMA2 from Hugging Face hub\u001b[39;00m\n\u001b[1;32m      5\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNousResearch/Llama-2-7b-chat-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-cpu/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-cpu/lib/python3.12/site-packages/transformers/modeling_utils.py:3398\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3395\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3398\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3400\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3401\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3402\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-cpu/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:62\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo GPU found. A GPU is needed for quantization.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install accelerate`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."
     ]
    }
   ],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=False)\n",
    "\n",
    "# Pre-trained LLAMA2 from Hugging Face hub\n",
    "base_model = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model, quantization_config=quant_config, device_map={\"\": 0})\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2dac05d-bf96-4a3d-b93b-4aba3f012c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examining model structure\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63dd6ceb-783d-4ff7-8f6f-dfbb8df1fb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is Ferdinand de Saussure?\n",
      " nobody is more important to the development of modern linguistics than Ferdinand de Saussure.\n",
      "Ferdinand de Saussure (1857-1913) was a Swiss linguist who is widely regarded as the father of modern linguistics. His work, particularly his book \"Course in General Linguistics,\" published in 1916, laid the groundwork for many of the key concepts and methods in\n"
     ]
    }
   ],
   "source": [
    "# generating text with a prompt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "prompt = \"Who is Ferdinand de Saussure?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "generate_ids = model.generate(inputs.input_ids.to(device), max_length=100)\n",
    "output = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a504a1f3-1ce2-4e12-bbb2-3bfb7cf78b9e",
   "metadata": {},
   "source": [
    "### Getting probabilities for the next word\n",
    "\n",
    "Based on [this Reddit thread](https://www.reddit.com/r/LocalLLaMA/comments/1b6xbg9/displayingreturning_probabilitieslogprobs_of_next/?rdt=37394)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ab2d88e-65dc-44e7-9565-bd1b8b4acb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2255: \"▁a\"\n",
      "0.0631: \"▁pleased\"\n",
      "0.0532: \"▁not\"\n",
      "0.0503: \"▁so\"\n",
      "0.0451: \"▁thr\"\n",
      "0.0417: \"▁an\"\n",
      "0.0315: \"▁writing\"\n",
      "0.0255: \"▁excited\"\n",
      "0.0203: \"▁grateful\"\n",
      "0.0184: \"▁happy\"\n"
     ]
    }
   ],
   "source": [
    "input_string = \"I am\"\n",
    "\n",
    "# tokenize input\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "\n",
    "# get model predictions (logits: values before they are turned into probabilities via softmax)\n",
    "logits = model(input_ids).logits\n",
    "\n",
    "# last projection: predicted next word after input\n",
    "logits = logits[-1, -1]\n",
    "\n",
    "# change logits to probabilities via softmax\n",
    "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "# top 10 most probable predicted words\n",
    "probs, ids = torch.topk(probs, 10)\n",
    "\n",
    "# convert token ids to text tokens\n",
    "texts = tokenizer.convert_ids_to_tokens(ids)\n",
    "\n",
    "# print probabilities + tokens\n",
    "for prob, text in zip(probs, texts):\n",
    "    print(f\"{prob:.4f}: \\\"{text}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6cb393-ffd5-47b9-b846-fefd017f0196",
   "metadata": {},
   "source": [
    "### Getting predictions from different Llama2 layers\n",
    "\n",
    "Based on this [code](https://github.com/nrimsky/LM-exp/blob/main/intermediate_decoding/intermediate_decoding.ipynb) with an associated [post](https://www.lesswrong.com/posts/fJE6tscjGRPnK8C2C/decoding-intermediate-activations-in-llama-2-7b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68b678d8-7554-4994-87cf-961b94c5edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnWrapper(torch.nn.Module):\n",
    "    def __init__(self, attn):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.activations = None\n",
    "        self.add_tensor = None\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.attn(*args, **kwargs)\n",
    "        if self.add_tensor is not None:\n",
    "            output = (output[0] + self.add_tensor,)+output[1:]\n",
    "        self.activations = output[0]\n",
    "        return output\n",
    "\n",
    "    def reset(self):\n",
    "        self.activations = None\n",
    "        self.add_tensor = None\n",
    "\n",
    "\n",
    "class BlockOutputWrapper(torch.nn.Module):\n",
    "    def __init__(self, block, unembed_matrix, norm):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.unembed_matrix = unembed_matrix\n",
    "        self.norm = norm\n",
    "\n",
    "        self.block.self_attn = AttnWrapper(self.block.self_attn)\n",
    "        self.post_attention_layernorm = self.block.post_attention_layernorm\n",
    "\n",
    "        self.attn_mech_output_unembedded = None\n",
    "        self.intermediate_res_unembedded = None\n",
    "        self.mlp_output_unembedded = None\n",
    "        self.block_output_unembedded = None\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.block(*args, **kwargs)\n",
    "        self.block_output_unembedded = self.unembed_matrix(self.norm(output[0]))\n",
    "        attn_output = self.block.self_attn.activations\n",
    "        self.attn_mech_output_unembedded = self.unembed_matrix(self.norm(attn_output))\n",
    "        attn_output += args[0]\n",
    "        self.intermediate_res_unembedded = self.unembed_matrix(self.norm(attn_output))\n",
    "        mlp_output = self.block.mlp(self.post_attention_layernorm(attn_output))\n",
    "        self.mlp_output_unembedded = self.unembed_matrix(self.norm(mlp_output))\n",
    "        return output\n",
    "\n",
    "    def attn_add_tensor(self, tensor):\n",
    "        self.block.self_attn.add_tensor = tensor\n",
    "\n",
    "    def reset(self):\n",
    "        self.block.self_attn.reset()\n",
    "\n",
    "    def get_attn_activations(self):\n",
    "        return self.block.self_attn.activations\n",
    "\n",
    "\n",
    "class Llama7BHelper:\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        for i, layer in enumerate(self.model.model.layers):\n",
    "            self.model.model.layers[i] = BlockOutputWrapper(layer, self.model.lm_head, self.model.model.norm)\n",
    "\n",
    "    def generate_text(self, prompt, max_length=100):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        generate_ids = self.model.generate(inputs.input_ids.to(self.device), max_length=max_length)\n",
    "        return self.tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "    def get_logits(self, prompt):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "          logits = self.model(inputs.input_ids.to(self.device)).logits\n",
    "          return logits\n",
    "\n",
    "    def set_add_attn_output(self, layer, add_output):\n",
    "        self.model.model.layers[layer].attn_add_tensor(add_output)\n",
    "\n",
    "    def get_attn_activations(self, layer):\n",
    "        return self.model.model.layers[layer].get_attn_activations()\n",
    "\n",
    "    def reset_all(self):\n",
    "        for layer in self.model.model.layers:\n",
    "            layer.reset()\n",
    "            \n",
    "    def print_decoded_activations(self, decoded_activations, label, topk=10):\n",
    "        softmaxed = torch.nn.functional.softmax(decoded_activations[0][-1], dim=-1)\n",
    "        values, indices = torch.topk(softmaxed, topk)\n",
    "        probs_percent = [int(v * 100) for v in values.tolist()]\n",
    "        tokens = self.tokenizer.batch_decode(indices.unsqueeze(-1))\n",
    "        print(label, list(zip(tokens, probs_percent)))\n",
    "\n",
    "    def decode_all_layers(self, text, topk=10, print_attn_mech=True, print_intermediate_res=True, print_mlp=True, print_block=True):\n",
    "        self.get_logits(text)\n",
    "        for i, layer in enumerate(self.model.model.layers):\n",
    "            print(f'Layer {i}: Decoded intermediate outputs')\n",
    "            if print_attn_mech:\n",
    "                self.print_decoded_activations(layer.attn_mech_output_unembedded, 'Attention mechanism', topk=topk)\n",
    "            if print_intermediate_res:\n",
    "                self.print_decoded_activations(layer.intermediate_res_unembedded, 'Intermediate residual stream', topk=topk)\n",
    "            if print_mlp:\n",
    "                self.print_decoded_activations(layer.mlp_output_unembedded, 'MLP output', topk=topk)\n",
    "            if print_block:\n",
    "                self.print_decoded_activations(layer.block_output_unembedded, 'Block output', topk=topk)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c14c40e1-719e-4e8b-86da-93bb7bfd46a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper = Llama7BHelper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e3d40a4-337a-4269-8653-59cbf38c1269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: Decoded intermediate outputs\n",
      "Attention mechanism [('solution', 0), ('loose', 0), ('berga', 0), ('', 0), ('first', 0), ('Private', 0), ('itself', 0), ('applied', 0), ('h', 0), ('пута', 0)]\n",
      "Intermediate residual stream [('ery', 8), ('berga', 2), ('ো', 0), ('Fritz', 0), ('SM', 0), ('SM', 0), ('aram', 0), ('solution', 0), ('hill', 0), ('ći', 0)]\n",
      "MLP output [('Kontrola', 2), ('昌', 0), ('Sito', 0), ('崎', 0), ('пута', 0), ('prüfe', 0), ('阳', 0), ('乡', 0), ('mez', 0), ('bezeichneter', 0)]\n",
      "Block output [('ery', 6), ('berga', 0), ('пута', 0), ('昌', 0), ('ো', 0), ('mez', 0), ('Apple', 0), ('阳', 0), ('Kontrola', 0), ('Ě', 0)]\n",
      "\n",
      "Layer 1: Decoded intermediate outputs\n",
      "Attention mechanism [('références', 1), ('chev', 0), ('isz', 0), ('rot', 0), ('embar', 0), ('Насе', 0), ('imedia', 0), ('ˇ', 0), ('older', 0), ('réseau', 0)]\n",
      "Intermediate residual stream [('ery', 20), ('print', 1), ('Außer', 1), ('wich', 0), ('mez', 0), ('mark', 0), ('ija', 0), ('dear', 0), ('impression', 0), ('lá', 0)]\n",
      "MLP output [('apers', 2), ('mez', 1), ('screen', 0), ('Screen', 0), ('tml', 0), ('ock', 0), ('wich', 0), ('len', 0), ('house', 0), ('ville', 0)]\n",
      "Block output [('ery', 13), ('wich', 4), ('mark', 1), ('dear', 1), ('print', 0), ('mez', 0), ('len', 0), ('house', 0), ('multicol', 0), ('mez', 0)]\n",
      "\n",
      "Layer 2: Decoded intermediate outputs\n",
      "Attention mechanism [('aterra', 1), ('icture', 1), ('áj', 0), ('LCCN', 0), ('crew', 0), ('łow', 0), ('redirects', 0), ('ział', 0), ('hai', 0), ('aster', 0)]\n",
      "Intermediate residual stream [('wich', 8), ('ery', 7), ('stone', 1), ('mark', 1), ('mez', 1), ('dear', 0), ('Anto', 0), ('house', 0), ('print', 0), ('flex', 0)]\n",
      "MLP output [('ilde', 1), ('noise', 0), ('aris', 0), ('agens', 0), ('imit', 0), ('bil', 0), ('hus', 0), ('віт', 0), ('hab', 0), ('Ru', 0)]\n",
      "Block output [('ery', 12), ('wich', 4), ('mez', 1), ('stone', 0), ('dear', 0), ('house', 0), ('apers', 0), ('eye', 0), ('jack', 0), ('unya', 0)]\n",
      "\n",
      "Layer 3: Decoded intermediate outputs\n",
      "Attention mechanism [('Gé', 1), ('Summary', 1), ('Хронологија', 0), ('nitt', 0), ('ikz', 0), ('atra', 0), ('Hou', 0), ('terior', 0), ('lacht', 0), ('WT', 0)]\n",
      "Intermediate residual stream [('ery', 3), ('house', 0), ('wich', 0), ('mez', 0), ('back', 0), ('arrow', 0), ('stone', 0), ('þ', 0), ('lass', 0), ('painted', 0)]\n",
      "MLP output [('iser', 2), ('itel', 1), ('uns', 1), ('Portug', 0), ('erson', 0), ('Gem', 0), ('iso', 0), ('own', 0), ('Sund', 0), ('jon', 0)]\n",
      "Block output [('ery', 3), ('wich', 2), ('lass', 1), ('stone', 0), ('back', 0), ('house', 0), ('print', 0), ('ock', 0), ('lee', 0), ('itel', 0)]\n",
      "\n",
      "Layer 4: Decoded intermediate outputs\n",
      "Attention mechanism [('mi', 1), ('amba', 0), ('aterra', 0), ('oka', 0), ('Bay', 0), ('plug', 0), ('cible', 0), ('Sank', 0), ('Mi', 0), ('Ghost', 0)]\n",
      "Intermediate residual stream [('lass', 3), ('wich', 1), ('Bay', 0), ('Sar', 0), ('âtre', 0), ('gem', 0), ('istes', 0), ('narrow', 0), ('Sob', 0), ('shame', 0)]\n",
      "MLP output [('보', 5), ('vex', 2), ('arc', 2), ('sted', 1), ('alu', 1), ('ˇ', 1), ('ption', 0), ('--+', 0), ('site', 0), ('点', 0)]\n",
      "Block output [('wich', 3), ('话', 2), ('bru', 0), ('цен', 0), ('lee', 0), ('ish', 0), ('iono', 0), ('ClassLoader', 0), ('話', 0), ('ченко', 0)]\n",
      "\n",
      "Layer 5: Decoded intermediate outputs\n",
      "Attention mechanism [('erei', 4), ('esterni', 4), ('�', 2), ('ethe', 1), ('败', 0), ('rok', 0), ('ipage', 0), ('ǐ', 0), ('itul', 0), ('ilon', 0)]\n",
      "Intermediate residual stream [('话', 3), ('wich', 1), ('bru', 1), ('話', 0), ('ченко', 0), ('lee', 0), ('ClassLoader', 0), ('ish', 0), ('цен', 0), ('erei', 0)]\n",
      "MLP output [('vel', 2), ('Pen', 0), ('tera', 0), ('м', 0), ('Publish', 0), ('pen', 0), ('ге', 0), ('äck', 0), ('ERR', 0), ('eign', 0)]\n",
      "Block output [('wich', 2), ('ufen', 1), ('ifen', 1), ('erei', 1), ('thumb', 0), ('bru', 0), ('uso', 0), ('话', 0), ('eni', 0), ('hou', 0)]\n",
      "\n",
      "Layer 6: Decoded intermediate outputs\n",
      "Attention mechanism [('perty', 3), ('phon', 3), ('icode', 1), ('sigu', 0), ('öl', 0), ('ху', 0), ('raph', 0), ('ienne', 0), ('orum', 0), ('paste', 0)]\n",
      "Intermediate residual stream [('bru', 1), ('цен', 0), ('ioni', 0), ('ifen', 0), ('wich', 0), ('imation', 0), ('ufen', 0), ('uso', 0), ('erei', 0), ('laps', 0)]\n",
      "MLP output [('idel', 1), ('NR', 0), ('Mitchell', 0), ('rez', 0), ('cen', 0), ('asi', 0), ('ierra', 0), ('outer', 0), ('walls', 0), ('unix', 0)]\n",
      "Block output [('wich', 2), ('цен', 0), ('erei', 0), ('Bent', 0), ('Apple', 0), ('iev', 0), ('hou', 0), ('ouvel', 0), ('imation', 0), ('pent', 0)]\n",
      "\n",
      "Layer 7: Decoded intermediate outputs\n",
      "Attention mechanism [('vba', 0), ('zar', 0), ('зво', 0), ('rijk', 0), ('visual', 0), ('Bit', 0), ('удо', 0), ('arina', 0), ('vern', 0), ('ponse', 0)]\n",
      "Intermediate residual stream [('wich', 1), ('pent', 0), ('Bent', 0), ('цен', 0), ('ouvel', 0), ('mail', 0), ('hou', 0), ('bru', 0), ('laps', 0), ('erei', 0)]\n",
      "MLP output [('aucoup', 2), ('Cong', 1), ('aña', 0), ('thick', 0), ('cí', 0), ('Academy', 0), ('eles', 0), ('cock', 0), ('defin', 0), ('ги', 0)]\n",
      "Block output [('wich', 6), ('uso', 0), ('го', 0), ('ufen', 0), ('jpeg', 0), ('Apple', 0), ('цен', 0), ('apple', 0), ('hook', 0), ('Fuß', 0)]\n",
      "\n",
      "Layer 8: Decoded intermediate outputs\n",
      "Attention mechanism [('mor', 3), ('mor', 1), ('Cult', 1), ('wan', 1), ('ião', 0), ('scriptstyle', 0), ('ähr', 0), ('uta', 0), ('nou', 0), ('ymnas', 0)]\n",
      "Intermediate residual stream [('wich', 8), ('uso', 0), ('го', 0), ('clos', 0), ('pent', 0), ('imation', 0), ('ifen', 0), ('apple', 0), ('jpeg', 0), ('ufen', 0)]\n",
      "MLP output [('atura', 1), ('emet', 1), ('Sar', 0), ('Porto', 0), ('cho', 0), ('gov', 0), ('û', 0), ('信', 0), ('picture', 0), ('uni', 0)]\n",
      "Block output [('jpeg', 1), ('wich', 0), ('emet', 0), ('ef', 0), ('onk', 0), ('gi', 0), ('Sar', 0), ('mail', 0), ('uso', 0), ('chor', 0)]\n",
      "\n",
      "Layer 9: Decoded intermediate outputs\n",
      "Attention mechanism [('ECK', 3), ('Jenkins', 1), ('andas', 1), ('Days', 1), ('ąż', 1), ('aden', 0), ('LIM', 0), ('éc', 0), ('black', 0), ('tec', 0)]\n",
      "Intermediate residual stream [('gi', 1), ('jpeg', 0), ('uso', 0), ('wich', 0), ('ocal', 0), ('onk', 0), ('bl', 0), ('emit', 0), ('Sar', 0), ('istre', 0)]\n",
      "MLP output [('attan', 1), ('Zar', 1), ('kl', 1), ('fatal', 1), ('full', 0), ('aden', 0), ('anta', 0), ('сси', 0), ('map', 0), ('optional', 0)]\n",
      "Block output [('Zar', 1), ('flex', 1), ('anta', 0), ('ku', 0), ('Sar', 0), ('jpeg', 0), ('kre', 0), ('wich', 0), ('onk', 0), ('uso', 0)]\n",
      "\n",
      "Layer 10: Decoded intermediate outputs\n",
      "Attention mechanism [('È', 3), ('orum', 2), ('bay', 1), ('aget', 0), ('velocity', 0), ('Dean', 0), ('zew', 0), ('ALSE', 0), ('azar', 0), ('ername', 0)]\n",
      "Intermediate residual stream [('flex', 1), ('kre', 1), ('anta', 0), ('ku', 0), ('onk', 0), ('jpeg', 0), ('rare', 0), ('Sar', 0), ('uso', 0), ('Zar', 0)]\n",
      "MLP output [('favor', 2), ('mine', 1), ('White', 1), ('제', 1), ('Mine', 0), ('reb', 0), ('MY', 0), ('inkel', 0), ('ahr', 0), ('ccc', 0)]\n",
      "Block output [('Sar', 3), ('flex', 0), ('ku', 0), ('uso', 0), ('anta', 0), ('onk', 0), ('endo', 0), ('wich', 0), ('Mill', 0), ('bl', 0)]\n",
      "\n",
      "Layer 11: Decoded intermediate outputs\n",
      "Attention mechanism [('merk', 1), ('tor', 0), ('revol', 0), ('Gh', 0), ('rak', 0), ('оте', 0), ('tact', 0), ('reu', 0), ('Kin', 0), ('etzt', 0)]\n",
      "Intermediate residual stream [('Sar', 2), ('ku', 1), ('anta', 0), ('ifen', 0), ('wich', 0), ('Mill', 0), ('onk', 0), ('uso', 0), ('flex', 0), ('kre', 0)]\n",
      "MLP output [('wor', 3), ('ster', 2), ('hmen', 0), ('oure', 0), ('bank', 0), ('interests', 0), ('ument', 0), ('imore', 0), ('сли', 0), ('neh', 0)]\n",
      "Block output [('Sar', 3), ('wich', 1), ('cla', 0), ('Mill', 0), ('ku', 0), ('bl', 0), ('ifen', 0), ('patch', 0), ('onk', 0), ('Fun', 0)]\n",
      "\n",
      "Layer 12: Decoded intermediate outputs\n",
      "Attention mechanism [('avia', 1), ('育', 1), ('ggi', 1), ('Mail', 1), ('utto', 0), ('cust', 0), ('Bis', 0), ('appa', 0), ('ori', 0), ('FIX', 0)]\n",
      "Intermediate residual stream [('wich', 1), ('ku', 1), ('Sar', 1), ('cla', 1), ('bl', 0), ('ifen', 0), ('win', 0), ('Mac', 0), ('Mill', 0), ('meant', 0)]\n",
      "MLP output [('вид', 5), ('UMN', 0), ('ello', 0), ('born', 0), ('ennis', 0), ('bay', 0), ('Gün', 0), ('eed', 0), ('UTE', 0), ('гов', 0)]\n",
      "Block output [('cla', 2), ('wich', 1), ('Sar', 1), ('ug', 0), ('ifen', 0), ('clause', 0), ('meant', 0), ('ku', 0), ('kre', 0), ('bl', 0)]\n",
      "\n",
      "Layer 13: Decoded intermediate outputs\n",
      "Attention mechanism [('oltre', 2), ('Autres', 0), ('Gé', 0), ('lands', 0), ('enberg', 0), ('mul', 0), ('repeating', 0), ('esp', 0), ('femin', 0), ('enburg', 0)]\n",
      "Intermediate residual stream [('wich', 2), ('ug', 1), ('cla', 1), ('Sar', 0), ('ifen', 0), ('meant', 0), ('ku', 0), ('clause', 0), ('lands', 0), ('endo', 0)]\n",
      "MLP output [('iech', 2), ('edia', 1), ('Mittel', 0), ('ору', 0), ('nick', 0), ('Herzog', 0), ('rikt', 0), ('TY', 0), ('esses', 0), ('Medi', 0)]\n",
      "Block output [('wich', 4), ('Sar', 1), ('Kra', 1), ('kre', 0), ('wood', 0), ('Zar', 0), ('cla', 0), ('lands', 0), ('ug', 0), ('endo', 0)]\n",
      "\n",
      "Layer 14: Decoded intermediate outputs\n",
      "Attention mechanism [('Ej', 3), ('ct', 0), ('olf', 0), ('form', 0), ('oru', 0), ('libre', 0), ('pool', 0), ('Tem', 0), ('ctu', 0), ('No', 0)]\n",
      "Intermediate residual stream [('wich', 3), ('Sar', 1), ('kre', 1), ('endo', 0), ('Kra', 0), ('lands', 0), ('drum', 0), ('wood', 0), ('ele', 0), ('ug', 0)]\n",
      "MLP output [('col', 5), ('ery', 3), ('antry', 2), ('ish', 1), ('fach', 1), ('estr', 0), ('edy', 0), ('xsd', 0), ('esi', 0), ('Sky', 0)]\n",
      "Block output [('wich', 12), ('ish', 1), ('UC', 0), ('per', 0), ('ku', 0), ('bid', 0), ('wid', 0), ('iful', 0), ('fin', 0), ('Sar', 0)]\n",
      "\n",
      "Layer 15: Decoded intermediate outputs\n",
      "Attention mechanism [('UC', 1), ('kre', 1), ('rh', 1), ('gla', 0), ('pod', 0), ('vote', 0), ('Proposition', 0), ('ències', 0), ('agua', 0), ('ymen', 0)]\n",
      "Intermediate residual stream [('wich', 10), ('UC', 3), ('ish', 1), ('kre', 0), ('fin', 0), ('ku', 0), ('wid', 0), ('lbl', 0), ('Haz', 0), ('ifen', 0)]\n",
      "MLP output [('Footnote', 1), ('тет', 1), ('Bedeut', 0), ('là', 0), ('års', 0), ('inks', 0), ('merk', 0), ('StackTrace', 0), ('imoine', 0), ('Einzel', 0)]\n",
      "Block output [('wich', 19), ('UC', 2), ('ish', 1), ('hos', 0), ('spaces', 0), ('ery', 0), ('mail', 0), ('lbl', 0), ('Heinrich', 0), ('wid', 0)]\n",
      "\n",
      "Layer 16: Decoded intermediate outputs\n",
      "Attention mechanism [('eme', 1), ('Blues', 1), ('ols', 1), ('sem', 0), ('sem', 0), ('Bass', 0), ('Sem', 0), ('icol', 0), ('Chem', 0), ('blue', 0)]\n",
      "Intermediate residual stream [('wich', 24), ('UC', 3), ('hos', 0), ('ish', 0), ('spaces', 0), ('mail', 0), ('lbl', 0), ('eme', 0), ('lands', 0), ('fin', 0)]\n",
      "MLP output [('rug', 2), ('atus', 0), ('afka', 0), ('DA', 0), ('zes', 0), ('Ban', 0), ('vention', 0), ('ờ', 0), ('developer', 0), ('actor', 0)]\n",
      "Block output [('wich', 10), ('UC', 3), ('ish', 1), ('hos', 1), ('ery', 0), ('spaces', 0), ('fin', 0), ('apple', 0), ('wood', 0), ('eme', 0)]\n",
      "\n",
      "Layer 17: Decoded intermediate outputs\n",
      "Attention mechanism [('♦', 2), ('Pho', 1), (':@', 1), ('пор', 0), ('ゼ', 0), ('кая', 0), ('références', 0), ('punkt', 0), ('testing', 0), ('Ses', 0)]\n",
      "Intermediate residual stream [('wich', 9), ('UC', 4), ('hos', 1), ('ish', 0), ('fin', 0), ('traffic', 0), ('wood', 0), ('col', 0), ('ery', 0), ('HL', 0)]\n",
      "MLP output [('estra', 1), ('emas', 0), ('meno', 0), ('back', 0), ('emb', 0), ('olean', 0), ('ainer', 0), ('UNION', 0), ('dot', 0), ('̍', 0)]\n",
      "Block output [('wich', 14), ('UC', 6), ('ish', 5), ('ery', 2), ('fin', 0), ('azionale', 0), ('hos', 0), ('Ce', 0), ('dot', 0), ('HL', 0)]\n",
      "\n",
      "Layer 18: Decoded intermediate outputs\n",
      "Attention mechanism [('outer', 1), ('bres', 0), ('ombres', 0), ('trans', 0), ('res', 0), ('mez', 0), ('obi', 0), ('aca', 0), ('wand', 0), ('fred', 0)]\n",
      "Intermediate residual stream [('wich', 10), ('UC', 6), ('ish', 4), ('ery', 1), ('dot', 0), ('hos', 0), ('HL', 0), ('azionale', 0), ('kre', 0), ('invert', 0)]\n",
      "MLP output [('awa', 4), ('äs', 2), ('ery', 1), ('Bes', 1), ('Hell', 1), ('ску', 1), ('vice', 1), ('еде', 0), ('estaven', 0), ('ění', 0)]\n",
      "Block output [('wich', 19), ('ery', 9), ('ish', 7), ('UC', 6), ('dot', 0), ('eme', 0), ('eyes', 0), ('Bean', 0), ('ovis', 0), ('beans', 0)]\n",
      "\n",
      "Layer 19: Decoded intermediate outputs\n",
      "Attention mechanism [('od', 1), ('Person', 0), ('plex', 0), ('otrop', 0), ('Ress', 0), ('fg', 0), ('良', 0), ('dom', 0), ('oracle', 0), ('quer', 0)]\n",
      "Intermediate residual stream [('wich', 16), ('ery', 7), ('UC', 6), ('ish', 6), ('dot', 1), ('eme', 0), ('beans', 0), ('Ce', 0), ('eyes', 0), ('oshi', 0)]\n",
      "MLP output [('thumb', 18), ('buch', 2), ('erm', 0), ('lon', 0), ('Tamb', 0), ('zie', 0), ('оло', 0), ('est', 0), ('alet', 0), ('lik', 0)]\n",
      "Block output [('ish', 29), ('wich', 15), ('ery', 5), ('UC', 3), ('Ce', 0), ('beans', 0), ('house', 0), ('isen', 0), ('ce', 0), ('Bean', 0)]\n",
      "\n",
      "Layer 20: Decoded intermediate outputs\n",
      "Attention mechanism [('color', 75), ('color', 11), ('Color', 11), ('Color', 0), ('colors', 0), ('colored', 0), ('colors', 0), ('Colors', 0), ('colour', 0), ('色', 0)]\n",
      "Intermediate residual stream [('ish', 28), ('wich', 16), ('ery', 4), ('UC', 3), ('Ce', 0), ('light', 0), ('beans', 0), ('house', 0), ('isen', 0), ('transparent', 0)]\n",
      "MLP output [('gre', 2), ('jul', 0), ('vá', 0), ('green', 0), ('Green', 0), ('town', 0), ('Пу', 0), ('proper', 0), ('bel', 0), ('Ready', 0)]\n",
      "Block output [('ish', 40), ('wich', 7), ('UC', 2), ('ery', 2), ('spaces', 0), ('house', 0), ('Ce', 0), ('isen', 0), ('transparent', 0), ('ishes', 0)]\n",
      "\n",
      "Layer 21: Decoded intermediate outputs\n",
      "Attention mechanism [('color', 2), ('odon', 1), ('uba', 0), ('carbon', 0), ('Color', 0), ('arda', 0), ('forces', 0), ('obox', 0), ('aft', 0), ('wagen', 0)]\n",
      "Intermediate residual stream [('ish', 37), ('wich', 7), ('UC', 2), ('ery', 1), ('spaces', 0), ('house', 0), ('Ce', 0), ('light', 0), ('isen', 0), ('alg', 0)]\n",
      "MLP output [('len', 1), ('method', 1), ('Reset', 0), ('typeof', 0), ('Graph', 0), ('Graph', 0), ('üs', 0), ('�', 0), ('team', 0), ('law', 0)]\n",
      "Block output [('ish', 30), ('wich', 4), ('ery', 2), ('UC', 1), ('spaces', 0), ('jub', 0), ('field', 0), ('Bean', 0), ('light', 0), ('ough', 0)]\n",
      "\n",
      "Layer 22: Decoded intermediate outputs\n",
      "Attention mechanism [('ücke', 0), ('�', 0), ('enes', 0), ('pol', 0), ('red', 0), ('orf', 0), ('기', 0), ('dorf', 0), ('Route', 0), ('ß', 0)]\n",
      "Intermediate residual stream [('ish', 25), ('wich', 3), ('ery', 2), ('UC', 1), ('esi', 0), ('field', 0), ('spaces', 0), ('gr', 0), ('ces', 0), ('light', 0)]\n",
      "MLP output [('клад', 1), ('atif', 0), ('house', 0), ('gets', 0), ('dur', 0), ('dom', 0), ('efe', 0), ('hash', 0), ('поль', 0), ('trap', 0)]\n",
      "Block output [('ish', 16), ('wich', 2), ('house', 2), ('gr', 1), ('esi', 1), ('ery', 0), ('spaces', 0), ('alg', 0), ('UC', 0), ('ius', 0)]\n",
      "\n",
      "Layer 23: Decoded intermediate outputs\n",
      "Attention mechanism [('сов', 0), ('ров', 0), ('Попис', 0), ('aria', 0), ('agen', 0), ('maz', 0), ('Хронологија', 0), ('hina', 0), ('azar', 0), ('unda', 0)]\n",
      "Intermediate residual stream [('ish', 13), ('wich', 2), ('house', 1), ('gr', 1), ('ery', 1), ('spaces', 0), ('esi', 0), ('alg', 0), ('ough', 0), ('UC', 0)]\n",
      "MLP output [('kw', 8), ('ire', 1), ('sun', 1), ('Plant', 0), ('Otto', 0), ('бір', 0), ('lit', 0), ('ween', 0), ('iero', 0), ('aggio', 0)]\n",
      "Block output [('ish', 14), ('house', 4), ('wich', 1), ('spaces', 1), ('lit', 1), ('light', 0), ('esi', 0), ('dawn', 0), ('alg', 0), ('gr', 0)]\n",
      "\n",
      "Layer 24: Decoded intermediate outputs\n",
      "Attention mechanism [('color', 57), ('color', 10), ('colors', 8), ('Color', 4), ('colour', 2), ('Color', 1), ('colours', 1), ('colored', 1), ('Colors', 1), ('colors', 0)]\n",
      "Intermediate residual stream [('ish', 16), ('house', 3), ('spaces', 1), ('lit', 1), ('wich', 1), ('light', 1), ('alg', 0), ('esi', 0), ('gr', 0), ('dawn', 0)]\n",
      "MLP output [('host', 1), ('esi', 1), ('ur', 1), ('lá', 1), ('Gr', 0), ('host', 0), ('ilon', 0), ('herit', 0), ('idge', 0), ('perman', 0)]\n",
      "Block output [('ish', 11), ('house', 8), ('esi', 4), ('lit', 2), ('ies', 1), ('gr', 0), ('spaces', 0), ('back', 0), ('wich', 0), ('ough', 0)]\n",
      "\n",
      "Layer 25: Decoded intermediate outputs\n",
      "Attention mechanism [('scal', 1), ('uen', 1), ('omp', 0), ('Sans', 0), ('берг', 0), ('curs', 0), ('mun', 0), ('hath', 0), ('oct', 0), ('œ', 0)]\n",
      "Intermediate residual stream [('ish', 10), ('house', 6), ('esi', 4), ('lit', 1), ('ies', 1), ('gr', 1), ('spaces', 0), ('wich', 0), ('ough', 0), ('ery', 0)]\n",
      "MLP output [('gerufen', 1), ('/~', 1), ('Dynamic', 0), ('ič', 0), ('eerd', 0), ('Convert', 0), ('ös', 0), ('schen', 0), ('rvm', 0), ('ány', 0)]\n",
      "Block output [('ish', 5), ('house', 4), ('esi', 4), ('ery', 1), ('lit', 1), ('light', 1), ('wich', 1), ('gr', 0), ('oshi', 0), ('bean', 0)]\n",
      "\n",
      "Layer 26: Decoded intermediate outputs\n",
      "Attention mechanism [('only', 1), ('only', 1), ('Committee', 1), ('dt', 0), ('Ry', 0), ('stone', 0), ('Hou', 0), ('pc', 0), ('asa', 0), ('Bere', 0)]\n",
      "Intermediate residual stream [('ish', 3), ('esi', 3), ('house', 2), ('ery', 2), ('light', 1), ('lit', 0), ('bean', 0), ('las', 0), ('wich', 0), ('Dragon', 0)]\n",
      "MLP output [('Lib', 1), ('capital', 1), ('par', 0), ('em', 0), ('d', 0), ('Ax', 0), ('inha', 0), ('パ', 0), ('O', 0), ('пар', 0)]\n",
      "Block output [('ish', 4), ('esi', 2), ('house', 2), ('light', 1), ('las', 1), ('leaf', 0), ('thumb', 0), ('hell', 0), ('alg', 0), ('gr', 0)]\n",
      "\n",
      "Layer 27: Decoded intermediate outputs\n",
      "Attention mechanism [('undle', 2), ('że', 1), ('acia', 1), ('j', 1), ('aben', 0), ('mod', 0), ('Variable', 0), ('Braun', 0), ('ugust', 0), ('Ram', 0)]\n",
      "Intermediate residual stream [('ish', 4), ('esi', 3), ('house', 2), ('light', 1), ('las', 0), ('ery', 0), ('thumb', 0), ('leaf', 0), ('flash', 0), ('alg', 0)]\n",
      "MLP output [('eken', 0), ('Jahrh', 0), ('cub', 0), ('Оте', 0), ('bbi', 0), ('ela', 0), ('Mys', 0), ('Rat', 0), ('élé', 0), ('прави', 0)]\n",
      "Block output [('ish', 5), ('esi', 2), ('las', 2), ('house', 0), ('ois', 0), ('flash', 0), ('light', 0), ('back', 0), ('gef', 0), ('gr', 0)]\n",
      "\n",
      "Layer 28: Decoded intermediate outputs\n",
      "Attention mechanism [('P', 21), ('S', 5), ('East', 2), ('West', 1), ('Dé', 0), ('Don', 0), ('DA', 0), ('Dic', 0), ('DA', 0), ('DM', 0)]\n",
      "Intermediate residual stream [('ish', 5), ('las', 2), ('esi', 2), ('flash', 0), ('house', 0), ('light', 0), ('ois', 0), ('back', 0), ('Pages', 0), ('Dragon', 0)]\n",
      "MLP output [('istrzost', 2), ('hill', 1), ('Hill', 1), ('�', 0), ('religion', 0), ('grammar', 0), ('elle', 0), ('umerate', 0), ('essa', 0), ('uelle', 0)]\n",
      "Block output [('ish', 4), ('esi', 1), ('gr', 1), ('las', 1), ('Dragon', 0), ('back', 0), ('house', 0), ('light', 0), ('fluid', 0), ('flash', 0)]\n",
      "\n",
      "Layer 29: Decoded intermediate outputs\n",
      "Attention mechanism [('dry', 1), ('lish', 0), ('8', 0), ('Press', 0), ('uen', 0), ('Las', 0), ('press', 0), ('meister', 0), ('gr', 0), ('ribu', 0)]\n",
      "Intermediate residual stream [('ish', 3), ('gr', 1), ('las', 1), ('esi', 1), ('back', 0), ('light', 0), ('fluid', 0), ('flash', 0), ('Dragon', 0), ('house', 0)]\n",
      "MLP output [('ide', 58), ('Ide', 29), ('IDE', 4), ('ide', 4), ('ideas', 1), ('IDE', 0), ('idea', 0), ('иде', 0), ('ideal', 0), ('l', 0)]\n",
      "Block output [('ideas', 6), ('ide', 6), ('ish', 3), ('las', 3), ('ies', 1), ('idea', 1), ('IDE', 1), ('Ide', 1), ('IDE', 0), ('gr', 0)]\n",
      "\n",
      "Layer 30: Decoded intermediate outputs\n",
      "Attention mechanism [('[', 0), ('...', 0), ('buy', 0), ('...', 0), ('`', 0), ('mens', 0), ('por', 0), ('Љ', 0), ('Por', 0), ('aph', 0)]\n",
      "Intermediate residual stream [('ideas', 6), ('ide', 5), ('las', 2), ('ish', 2), ('ies', 1), ('Ide', 1), ('idea', 1), ('IDE', 1), ('IDE', 0), ('Las', 0)]\n",
      "MLP output [('T', 97), ('m', 0), ('tr', 0), ('V', 0), ('\\n', 0), ('(', 0), ('s', 0), ('H', 0), ('ch', 0), ('e', 0)]\n",
      "Block output [('ideas', 8), ('las', 5), ('ish', 4), ('ide', 4), ('-', 3), ('d', 2), ('g', 2), ('\\n', 2), ('.', 1), ('ies', 1)]\n",
      "\n",
      "Layer 31: Decoded intermediate outputs\n",
      "Attention mechanism [('...', 1), ('...', 1), ('Sym', 1), ('geomet', 0), ('b', 0), ('ones', 0), ('seconds', 0), ('abeth', 0), ('uch', 0), ('lob', 0)]\n",
      "Intermediate residual stream [('las', 9), ('ideas', 6), ('-', 5), ('.', 3), ('d', 3), ('\\n', 2), ('g', 2), ('ish', 2), ('ide', 2), (',', 1)]\n",
      "MLP output [('ideas', 2), ('thoughts', 2), ('elli', 2), ('medal', 2), ('hill', 1), ('eggs', 1), ('curl', 1), ('hills', 0), ('clouds', 0), ('epo', 0)]\n",
      "Block output [('ideas', 72), ('d', 1), ('is', 1), ('-', 0), ('.', 0), ('g', 0), ('and', 0), ('las', 0), ('eggs', 0), (',', 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "helper.decode_all_layers('Colorless green')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
