{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c4f0e8-c319-4ea1-a73b-8c4866cf639f",
   "metadata": {},
   "source": [
    "# Notebook for LLM workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3194a575-8f3d-464e-9b4d-cd0669f8555a",
   "metadata": {},
   "source": [
    "## Part 1. Examining and fine-tuning BERT\n",
    "\n",
    "Based on a tutorial from [Kaggle](https://www.kaggle.com/code/harshjain123/bert-for-everyone-tutorial-implementation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780624af-77f9-4f70-9c6b-a741fbaa11ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe851d2a-895a-424a-a10d-1cffccd2262b",
   "metadata": {},
   "source": [
    "### Getting BERT and its tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8890e8f-7617-400b-9e8e-5cad32af95ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Transformers library and download BERT\n",
    "\n",
    "from transformers import AutoModel, BertTokenizerFast, AdamW\n",
    "\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf4536a-0b3f-49d5-a1f9-729426213692",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce4e5a-9693-4090-949a-cc8caa2be785",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dda3af-0ace-429c-8073-4a7949174c6a",
   "metadata": {},
   "source": [
    "### Defining the BERT-based sentence classifier architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8d0c1-a76d-4472-aaf6-ea7afa7005da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        # BERT part\n",
    "        self.bert = bert \n",
    "        \n",
    "        # dropout layer (random removing of components during training for improving performance)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    # define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        \n",
    "        # pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "\n",
    "        # run BERT output through dense layer 1\n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        # apply relu activation function\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # apply dropout\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "      \n",
    "        # apply softmax activation (get probability distribution across target classes)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e9741-d715-41ba-a415-36b9f71e3ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_Arch(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb0f03-8c7d-4d32-8ff0-cd736c3c2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put model to GPU if available\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f3c23-029d-4b3d-adb5-3ef6e202eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018fb65b-0324-4cf2-b31b-f8c112cd0cae",
   "metadata": {},
   "source": [
    "### Getting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d5b3d3-6af3-47cb-9dcb-597cbc9aa212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pre-labeled spam detection dataset\n",
    "df = pd.read_csv(\"spamdata_v2.csv\")\n",
    "\n",
    "# split train dataset into train, validation and test sets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
    "                                                                    random_state=2024, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['label'])\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2024, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)\n",
    "\n",
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 25,\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = 25,\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 25,\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e365c12b-cc3c-4d5d-8b8a-df0a02ba1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lists to tensors\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f44f3-0978-474a-b284-a181551b8045",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_mask is used for ignoring input padding in attention\n",
    "\n",
    "print(tokenizer.decode(train_seq[0]))\n",
    "print(train_mask[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60782afc-275a-4fce-9701-793e82414372",
   "metadata": {},
   "source": [
    "### Training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a40a2-f1c9-43a6-a9ec-63c1b6f0bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors for training data\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for randomly sampling data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for training data\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors for validation data\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for randomly sampling data during validation\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880c18ed-64cc-434c-9411-d091bab58290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all BERT parameters (training only linear layers outside of BERT)\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93a2932-25e6-43a3-914b-5187f9da4303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),lr = 1e-5) \n",
    "\n",
    "#compute the class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "\n",
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# push to GPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58916c01-e521-49c1-8f6a-0d9a18770627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "\n",
    "    # put model in training mode (apply Dropout)\n",
    "    model.train()\n",
    "\n",
    "    # initialize loss and accuracy\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save model predictions\n",
    "    total_preds = []\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # print progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        \n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    " \n",
    "        sent_id, mask, labels = batch\n",
    "        \n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "      # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds\n",
    "\n",
    "\n",
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    \n",
    "    print(\"\\nEvaluating...\")\n",
    "  \n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "        \n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            \n",
    "            # # Calculate elapsed time in minutes.\n",
    "            # elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca204be-ab2e-4d92-a386-f5c15ab2918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "# for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    # train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    # evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    # save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')\n",
    "\n",
    "\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))\n",
    "\n",
    "# free cached GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d5ac5-1b5f-4847-ae74-ef317a5cdeb9",
   "metadata": {},
   "source": [
    "## Part 2. Sentence similarities with SentenceBERT\n",
    "\n",
    "Based on [SentenceBERT documentation](https://sbert.net/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54327ee4-45b3-4ae4-aff4-4eb39695524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c5817d-5f91-4cab-85a3-8c969b1d954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa19710-0d50-49c0-874f-2bc0e2d2e3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences to encode\n",
    "sentences = [\n",
    "    \"I am happy.\",\n",
    "    \"I am sad.\",\n",
    "    \"I am content.\",\n",
    "    \"I am not happy.\",\n",
    "    \"I am not sad.\"\n",
    "]\n",
    "\n",
    "# embeddings\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# embedding similarities\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a102f91-2805-4443-8b24-de3f729fbeef",
   "metadata": {},
   "source": [
    "## Part 3. Examining Llama2 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8452ce57-1fec-4605-b043-b985d8152034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd9574b-89b2-4b82-9b64-15abb47038d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=compute_dtype, bnb_4bit_use_double_quant=False)\n",
    "\n",
    "# Pre-trained LLAMA2 from Hugging Face hub\n",
    "base_model = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model, quantization_config=quant_config, device_map={\"\": 0})\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dac05d-bf96-4a3d-b93b-4aba3f012c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examining model structure\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dd6ceb-783d-4ff7-8f6f-dfbb8df1fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating text with a prompt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "prompt = \"Who is Ferdinand de Saussure?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "generate_ids = model.generate(inputs.input_ids.to(device), max_length=100)\n",
    "output = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a504a1f3-1ce2-4e12-bbb2-3bfb7cf78b9e",
   "metadata": {},
   "source": [
    "### Getting probabilities for the next word\n",
    "\n",
    "Based on [this Reddit thread](https://www.reddit.com/r/LocalLLaMA/comments/1b6xbg9/displayingreturning_probabilitieslogprobs_of_next/?rdt=37394)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab2d88e-65dc-44e7-9565-bd1b8b4acb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = \"I am\"\n",
    "\n",
    "# tokenize input\n",
    "input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "\n",
    "# get model predictions (logits: values before they are turned into probabilities via softmax)\n",
    "logits = model(input_ids).logits\n",
    "\n",
    "# last projection: predicted next word after input\n",
    "logits = logits[-1, -1]\n",
    "\n",
    "# change logits to probabilities via softmax\n",
    "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "# top 10 most probable predicted words\n",
    "probs, ids = torch.topk(probs, 10)\n",
    "\n",
    "# convert token ids to text tokens\n",
    "texts = tokenizer.convert_ids_to_tokens(ids)\n",
    "\n",
    "# print probabilities + tokens\n",
    "for prob, text in zip(probs, texts):\n",
    "    print(f\"{prob:.4f}: \\\"{text}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6cb393-ffd5-47b9-b846-fefd017f0196",
   "metadata": {},
   "source": [
    "### Getting predictions from different Llama2 layers\n",
    "\n",
    "Based on this [code](https://github.com/nrimsky/LM-exp/blob/main/intermediate_decoding/intermediate_decoding.ipynb) with an associated [post](https://www.lesswrong.com/posts/fJE6tscjGRPnK8C2C/decoding-intermediate-activations-in-llama-2-7b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b678d8-7554-4994-87cf-961b94c5edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnWrapper(torch.nn.Module):\n",
    "    def __init__(self, attn):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.activations = None\n",
    "        self.add_tensor = None\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.attn(*args, **kwargs)\n",
    "        if self.add_tensor is not None:\n",
    "            output = (output[0] + self.add_tensor,)+output[1:]\n",
    "        self.activations = output[0]\n",
    "        return output\n",
    "\n",
    "    def reset(self):\n",
    "        self.activations = None\n",
    "        self.add_tensor = None\n",
    "\n",
    "\n",
    "class BlockOutputWrapper(torch.nn.Module):\n",
    "    def __init__(self, block, unembed_matrix, norm):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.unembed_matrix = unembed_matrix\n",
    "        self.norm = norm\n",
    "\n",
    "        self.block.self_attn = AttnWrapper(self.block.self_attn)\n",
    "        self.post_attention_layernorm = self.block.post_attention_layernorm\n",
    "\n",
    "        self.attn_mech_output_unembedded = None\n",
    "        self.intermediate_res_unembedded = None\n",
    "        self.mlp_output_unembedded = None\n",
    "        self.block_output_unembedded = None\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.block(*args, **kwargs)\n",
    "        self.block_output_unembedded = self.unembed_matrix(self.norm(output[0]))\n",
    "        attn_output = self.block.self_attn.activations\n",
    "        self.attn_mech_output_unembedded = self.unembed_matrix(self.norm(attn_output))\n",
    "        attn_output += args[0]\n",
    "        self.intermediate_res_unembedded = self.unembed_matrix(self.norm(attn_output))\n",
    "        mlp_output = self.block.mlp(self.post_attention_layernorm(attn_output))\n",
    "        self.mlp_output_unembedded = self.unembed_matrix(self.norm(mlp_output))\n",
    "        return output\n",
    "\n",
    "    def attn_add_tensor(self, tensor):\n",
    "        self.block.self_attn.add_tensor = tensor\n",
    "\n",
    "    def reset(self):\n",
    "        self.block.self_attn.reset()\n",
    "\n",
    "    def get_attn_activations(self):\n",
    "        return self.block.self_attn.activations\n",
    "\n",
    "\n",
    "class Llama7BHelper:\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        for i, layer in enumerate(self.model.model.layers):\n",
    "            self.model.model.layers[i] = BlockOutputWrapper(layer, self.model.lm_head, self.model.model.norm)\n",
    "\n",
    "    def generate_text(self, prompt, max_length=100):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        generate_ids = self.model.generate(inputs.input_ids.to(self.device), max_length=max_length)\n",
    "        return self.tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "    def get_logits(self, prompt):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "          logits = self.model(inputs.input_ids.to(self.device)).logits\n",
    "          return logits\n",
    "\n",
    "    def set_add_attn_output(self, layer, add_output):\n",
    "        self.model.model.layers[layer].attn_add_tensor(add_output)\n",
    "\n",
    "    def get_attn_activations(self, layer):\n",
    "        return self.model.model.layers[layer].get_attn_activations()\n",
    "\n",
    "    def reset_all(self):\n",
    "        for layer in self.model.model.layers:\n",
    "            layer.reset()\n",
    "            \n",
    "    def print_decoded_activations(self, decoded_activations, label, topk=10):\n",
    "        softmaxed = torch.nn.functional.softmax(decoded_activations[0][-1], dim=-1)\n",
    "        values, indices = torch.topk(softmaxed, topk)\n",
    "        probs_percent = [int(v * 100) for v in values.tolist()]\n",
    "        tokens = self.tokenizer.batch_decode(indices.unsqueeze(-1))\n",
    "        print(label, list(zip(tokens, probs_percent)))\n",
    "\n",
    "    def decode_all_layers(self, text, topk=10, print_attn_mech=True, print_intermediate_res=True, print_mlp=True, print_block=True):\n",
    "        self.get_logits(text)\n",
    "        for i, layer in enumerate(self.model.model.layers):\n",
    "            print(f'Layer {i}: Decoded intermediate outputs')\n",
    "            if print_attn_mech:\n",
    "                self.print_decoded_activations(layer.attn_mech_output_unembedded, 'Attention mechanism', topk=topk)\n",
    "            if print_intermediate_res:\n",
    "                self.print_decoded_activations(layer.intermediate_res_unembedded, 'Intermediate residual stream', topk=topk)\n",
    "            if print_mlp:\n",
    "                self.print_decoded_activations(layer.mlp_output_unembedded, 'MLP output', topk=topk)\n",
    "            if print_block:\n",
    "                self.print_decoded_activations(layer.block_output_unembedded, 'Block output', topk=topk)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14c40e1-719e-4e8b-86da-93bb7bfd46a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper = Llama7BHelper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3d40a4-337a-4269-8653-59cbf38c1269",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.decode_all_layers('Colorless green')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
